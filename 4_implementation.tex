\section{実装}


3章で述べたシステム設計をもとにして，プロジェクションマッピングによる情報提示を行うプロトタイプデバイスを実装した．デバイスの外観を図に示す．デバイスの重さは約530gであり，このデバイスを読書中に首からかけて使用することで，機能を使用する．デバイスを使用している様子を図に示す．なおプロジェクタ・ウェブカメラの部分の角度は図のように可変となっており，読書時の姿勢に応じて手動で調整する．ウェブカメラにはMicrosoft社のLifeCam Cinema，プロジェクタにはTenker社のDLP mini projectorを用いた．プロジェクタの投射距離は18〜250cmであり，最大100インチでの投影が可能である．また本デバイスは本から直線距離で約30cm程離して利用することを想定しており，その際のプロジェクタの照射可能範囲は縦約22cm，横約14cmである．同様にカメラのカバー範囲は縦約21cm，横約33cmである．これを図に示す．またデバイスの照射可能距離を図に示す．ウェブカメラ，ハンドジェスチャ認識デバイスはそれぞれPCにUSB接続されている．ハンドジェスチャ認識デバイスのジェスチャをカバーする範囲は，本体を中心に縦約50cm，横約45cmである．なおPCはApple社のMacBook Pro (CPU: Core i5，メモリ: 8GB)を用いた．ソフトウェアに関しては，OpenCVやCloud Vision API，MeCabなどの処理をpythonで行い，プロジェクションマッピングによる描画の処理にはProcessingを用いた．OpenCVとは，オープンソースのコンピュータビジョンライブラリであり，Cloud Vision APIとは，機械学習モデルを用いて画像の内容を取得する画像分析APIである．


\subsection{紙面のトラッキング}
紙面のトラッキングの際には，OpenCVを用いたトラッキングを行った\cite{opencv}．OpenCVで使用できるトラッキングアルゴリズムには，Boosting，MIL，TLD，MedianFlow，KCFの5つが存在するが，リアルタイムで精度の高い認識が求められるため，今回はこの中でも高精度かつ高速に物体を学習し追跡できるKCFを用いた．

\subsection{システムへの入力}

本システムへユーザが入力を行う際は，LeapMotion\cite{leap}によって手の動作を認識することによって行った．LeapMotionとは手のジェスチャを認識することで，コンピュータへの入力を可能とする入力機器である．このデバイスでは，手のひらの中央および各指，各関節の始点と終点を3次元の座標と方向ベクトルで取得できる．またcircle，swipe，key tap，screen tapの4種のジェスチャを認識できる．各ジェスチャを図に示す．本研究では，人差し指の3次元座標とswipe，key tapジェスチャを用いる．ユーザが必要な情報を得たい場合は，key tapジェスチャを認識することにより機能を実行できる．また表示されている名詞を選択する際は，ページの遷移をswipeジェスチャで行い，指定の名詞を人差し指の3次元座標とkey tapジェスチャによって選択する．また検索結果を削除する際もkey tapジェスチャによって行った．


\subsection{文字認識}

読書中のページの文章を取得する．デバイスのウェブカメラから紙面を撮像し，LeapMotionにより指のkey tapジェスチャを認識した際にその時点でのカメラが撮像している画像を取得する．この取得した画像をPCで処理することにより，文字認識を行った．取得した画像には読書中の本の紙面が含まれているため，その画像から光学文字認識により文章を取得する．この際にGoogleのCloud Vision API\cite{google}を使用した．このAPIを用いて検出できた文章に対し，次項で述べる形態素解析を行った．

\subsection{形態素解析}

文字認識によって取得した文章を日本語形態素解析機MeCab\cite{mecab}によって形態素解析した．
形態素解析とは，自然言語によって記述された文章を，対象言語の文法や辞書の情報に基づき，文章の最小単位である形態素に分割し，それぞれの形態素の分析を行うことである．
形態素に分けた文章から名詞のみを抽出し，プロジェクタにより紙面に表示する．この際に，紙面の文章と表示した名詞とが重なり可読性が下がることを避けるため，紙面上部の余白領域に名詞を表示するようにした．

